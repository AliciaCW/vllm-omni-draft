#!/usr/bin/env python3
"""
vLLM é›†æˆæµ‹è¯•è„šæœ¬ï¼šå®Œæ•´çš„ QwenImage ç”Ÿæˆæµç¨‹

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºäº†å¦‚ä½•åœ¨å®é™…çš„ vLLM ç¯å¢ƒä¸­ä½¿ç”¨ QwenImage ç”Ÿæˆå›¾åƒã€‚
æ³¨æ„ï¼šè¿™éœ€è¦å®é™…çš„æ¨¡å‹æ–‡ä»¶å’Œå®Œæ•´çš„ vLLM ç¯å¢ƒã€‚
"""

from qwen_image_gen.types import (
    QwenImageInputs,
    QwenImageTask,
    QwenImageOutputMode
)
from qwen_image_gen import create_qwen_image_config
import asyncio
import os
import sys
import torch
import numpy as np
from PIL import Image
from typing import Optional, Dict, Any

# æ·»åŠ  src ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

try:
    from vllm import AsyncLLM
    from vllm.config import VllmConfig
    from vllm.sampling_params import SamplingParams
except ImportError:
    print("âŒ vLLM æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…: pip install vllm")
    sys.exit(1)


class QwenImageVLLMTester:
    """QwenImage vLLM é›†æˆæµ‹è¯•å™¨"""

    def __init__(self):
        self.engine = None
        self.config = None

    async def setup(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        print("ğŸ”§ è®¾ç½® vLLM é›†æˆç¯å¢ƒ...")

        # åˆ›å»ºé…ç½®
        self.config = create_qwen_image_config(
            transformer_model_id="Qwen/QwenImage-1.5B",
            vae_model_id="Qwen/QwenImage-VAE",
            max_batch_size=1,  # å°æ‰¹æ¬¡ç”¨äºæµ‹è¯•
            default_height=512,
            default_width=512,
            enable_debug=True
        )

        print(f"âœ… é…ç½®åˆ›å»ºå®Œæˆ: {self.config.transformer_model_id}")

        # åˆ›å»º vLLM é…ç½®
        vllm_config_dict = self.config.to_vllm_config()
        vllm_config = VllmConfig(**vllm_config_dict)

        print("âœ… vLLM é…ç½®åˆ›å»ºå®Œæˆ")

        # åˆå§‹åŒ–å¼•æ“
        try:
            print("ğŸš€ åˆå§‹åŒ– vLLM å¼•æ“...")
            self.engine = AsyncLLM.from_vllm_config(
                vllm_config=vllm_config,
                executor_class="qwen_image_gen.executor.QwenImageGenExecutor"
            )
            print("âœ… vLLM å¼•æ“åˆå§‹åŒ–æˆåŠŸ")

        except Exception as e:
            print(f"âŒ vLLM å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            print("ğŸ’¡ å¯èƒ½çš„åŸå› :")
            print("   1. æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— æ³•ä¸‹è½½")
            print("   2. å†…å­˜ä¸è¶³")
            print("   3. CUDA ç¯å¢ƒé—®é¢˜")
            print("   4. diffusers æœªå®‰è£…")
            raise

    async def test_text_to_image(self, prompt: str = "A beautiful sunset over mountains"):
        """æµ‹è¯•æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ"""
        print(f"\nğŸ¨ æµ‹è¯•æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ: '{prompt}'")

        try:
            # åˆ›å»ºè¾“å…¥æ•°æ®
            inputs = await self._create_text_to_image_inputs(prompt)

            # ç”Ÿæˆå›¾åƒ
            print("ğŸ”„ å¼€å§‹ç”Ÿæˆå›¾åƒ...")
            result = await self._generate_image(inputs)

            if result is not None:
                print("âœ… å›¾åƒç”ŸæˆæˆåŠŸ")
                return result
            else:
                print("âŒ å›¾åƒç”Ÿæˆå¤±è´¥")
                return None

        except Exception as e:
            print(f"âŒ æ–‡æœ¬åˆ°å›¾åƒæµ‹è¯•å¤±è´¥: {e}")
            return None

    async def _create_text_to_image_inputs(self, prompt: str) -> QwenImageInputs:
        """åˆ›å»ºæ–‡æœ¬åˆ°å›¾åƒçš„è¾“å…¥"""
        print("ğŸ“ åˆ›å»ºæ–‡æœ¬åˆ°å›¾åƒè¾“å…¥...")

        # åˆ›å»ºæ¨¡æ‹Ÿçš„æ–‡æœ¬åµŒå…¥
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥ä½¿ç”¨æ–‡æœ¬ç¼–ç å™¨
        batch_size = 1
        seq_len = 77
        embed_dim = 768

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        dtype = torch.float16 if torch.cuda.is_available() else torch.float32

        # æ¨¡æ‹Ÿ CLIP é£æ ¼çš„åµŒå…¥
        prompt_embeds = torch.randn(
            batch_size, seq_len, embed_dim,
            device=device, dtype=dtype
        )

        prompt_mask = torch.ones(
            batch_size, seq_len,
            device=device, dtype=torch.bool
        )

        # åˆ›å»ºåˆå§‹å™ªå£°
        latent_channels = 4
        latent_height = self.config.default_height // 8
        latent_width = self.config.default_width // 8

        image_latents = torch.randn(
            batch_size, latent_channels, latent_height, latent_width,
            device=device, dtype=dtype
        )

        # åˆ›å»ºæ—¶é—´æ­¥
        timesteps = torch.linspace(
            1.0, 0.0, steps=self.config.default_num_inference_steps, device=device)
        timesteps = (timesteps * 1000).long()

        # åˆ›å»º QwenImage è¾“å…¥
        inputs = QwenImageInputs(
            prompt_embeds=prompt_embeds,
            prompt_embeds_mask=prompt_mask,
            image_latents=image_latents,
            timesteps=timesteps,
            guidance_scale=self.config.default_guidance_scale,
            num_inference_steps=self.config.default_num_inference_steps,
            task=QwenImageTask.TEXT_TO_IMAGE,
            output_mode=QwenImageOutputMode.PIXELS
        )

        print(f"âœ… è¾“å…¥åˆ›å»ºå®Œæˆ: {inputs.image_latents.shape}")
        return inputs

    async def _generate_image(self, inputs: QwenImageInputs) -> Optional[torch.Tensor]:
        """ç”Ÿæˆå›¾åƒ"""
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨ vLLM çš„ç”Ÿæˆæ–¹æ³•
            # ç”±äºæˆ‘ä»¬çš„å®ç°è¿˜åœ¨å¼€å‘ä¸­ï¼Œè¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿç”Ÿæˆ

            print("ğŸ”„ æ¨¡æ‹Ÿå›¾åƒç”Ÿæˆè¿‡ç¨‹...")

            # è·å–åˆå§‹æ½œåœ¨è¡¨ç¤º
            latents = inputs.image_latents.clone()

            # æ¨¡æ‹Ÿå»å™ªè¿‡ç¨‹
            for i in range(inputs.num_inference_steps):
                step = i + 1
                if step % 5 == 0:  # æ¯5æ­¥æ‰“å°ä¸€æ¬¡è¿›åº¦
                    print(f"   æ­¥éª¤ {step}/{inputs.num_inference_steps}")

                # æ¨¡æ‹Ÿå™ªå£°å‡å°‘
                noise_scale = 1.0 - (step / inputs.num_inference_steps)
                noise = torch.randn_like(latents) * noise_scale * 0.1
                latents = latents - noise

            # æ¨¡æ‹Ÿ VAE è§£ç 
            print("ğŸ–¼ï¸  æ¨¡æ‹Ÿ VAE è§£ç ...")
            batch_size, channels, height, width = latents.shape

            # ä¸Šé‡‡æ ·åˆ°åƒç´ ç©ºé—´
            pixel_height = height * 8
            pixel_width = width * 8
            pixel_channels = 3  # RGB

            pixels = torch.randn(
                batch_size, pixel_channels, pixel_height, pixel_width,
                device=latents.device, dtype=latents.dtype
            )

            # å½’ä¸€åŒ–åˆ° [0, 1] èŒƒå›´
            pixels = (pixels - pixels.min()) / (pixels.max() - pixels.min())

            print(f"âœ… ç”Ÿæˆå®Œæˆ: {pixels.shape}")
            return pixels

        except Exception as e:
            print(f"âŒ å›¾åƒç”Ÿæˆå¤±è´¥: {e}")
            return None

    def save_image(self, pixels: torch.Tensor, filename: str = "vllm_generated_image.png"):
        """ä¿å­˜ç”Ÿæˆçš„å›¾åƒ"""
        print(f"ğŸ’¾ ä¿å­˜å›¾åƒ: {filename}")

        try:
            # è½¬æ¢ä¸º numpy æ•°ç»„
            if pixels.dim() == 4:
                pixels = pixels[0]  # å–ç¬¬ä¸€ä¸ªæ‰¹æ¬¡

            # è½¬æ¢ä¸º [H, W, C] æ ¼å¼
            pixels = pixels.permute(1, 2, 0).cpu().numpy()

            # ç¡®ä¿å€¼åœ¨ [0, 1] èŒƒå›´å†…
            pixels = np.clip(pixels, 0, 1)

            # è½¬æ¢ä¸º [0, 255] èŒƒå›´
            pixels = (pixels * 255).astype(np.uint8)

            # åˆ›å»º PIL å›¾åƒ
            image = Image.fromarray(pixels)

            # ä¿å­˜å›¾åƒ
            image.save(filename)

            print(f"âœ… å›¾åƒä¿å­˜æˆåŠŸ: {filename}")
            print(f"   å›¾åƒå°ºå¯¸: {image.size}")

        except Exception as e:
            print(f"âŒ å›¾åƒä¿å­˜å¤±è´¥: {e}")

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.engine is not None:
            print("ğŸ§¹ æ¸…ç† vLLM å¼•æ“...")
            try:
                if hasattr(self.engine, 'shutdown'):
                    await self.engine.shutdown()
                print("âœ… å¼•æ“æ¸…ç†å®Œæˆ")
            except Exception as e:
                print(f"âš ï¸  å¼•æ“æ¸…ç†è­¦å‘Š: {e}")


async def run_vllm_integration_test():
    """è¿è¡Œ vLLM é›†æˆæµ‹è¯•"""
    print("ğŸš€ å¼€å§‹ vLLM é›†æˆæµ‹è¯•")
    print("=" * 60)

    tester = QwenImageVLLMTester()

    try:
        # è®¾ç½®ç¯å¢ƒ
        await tester.setup()

        # æµ‹è¯•æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ
        result = await tester.test_text_to_image("A beautiful sunset over mountains")

        if result is not None:
            # ä¿å­˜ç»“æœ
            tester.save_image(result, "vllm_test_result.png")

            print("\nğŸ‰ vLLM é›†æˆæµ‹è¯•æˆåŠŸï¼")
            print("ğŸ“‹ æµ‹è¯•ç»“æœ:")
            print(f"   - ç”Ÿæˆå›¾åƒå°ºå¯¸: {result.shape}")
            print(f"   - ä¿å­˜æ–‡ä»¶: vllm_test_result.png")
        else:
            print("\nâŒ vLLM é›†æˆæµ‹è¯•å¤±è´¥")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("ğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("   1. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        print("   2. ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å­˜")
        print("   3. æ£€æŸ¥ CUDA ç¯å¢ƒ")
        print("   4. å®‰è£…å¿…è¦çš„ä¾èµ–")

    finally:
        # æ¸…ç†èµ„æº
        await tester.cleanup()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ QwenImage vLLM é›†æˆæµ‹è¯•")
    print("=" * 60)

    # æ£€æŸ¥ç¯å¢ƒ
    print("ğŸ” æ£€æŸ¥ç¯å¢ƒ...")

    if not torch.cuda.is_available():
        print("âš ï¸  è­¦å‘Š: CUDA ä¸å¯ç”¨ï¼Œæµ‹è¯•å¯èƒ½å¾ˆæ…¢")

    try:
        import diffusers
        print(f"âœ… diffusers ç‰ˆæœ¬: {diffusers.__version__}")
    except ImportError:
        print("âŒ diffusers æœªå®‰è£…ï¼Œè¯·å®‰è£…: pip install diffusers")
        return

    try:
        import vllm
        print(f"âœ… vLLM ç‰ˆæœ¬: {vllm.__version__}")
    except ImportError:
        print("âŒ vLLM æœªå®‰è£…ï¼Œè¯·å®‰è£…: pip install vllm")
        return

    # è¿è¡Œæµ‹è¯•
    asyncio.run(run_vllm_integration_test())


if __name__ == "__main__":
    main()
