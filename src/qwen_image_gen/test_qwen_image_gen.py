#!/usr/bin/env python3
"""
æµ‹è¯•è„šæœ¬ï¼šQwenImage Generation with vLLM v1

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨æ–°è®¾è®¡çš„ qwen_image_gen åŒ…æ¥ç”Ÿæˆå›¾åƒã€‚
"""

from qwen_image_gen.types import (
    QwenImageInputs,
    QwenImageTask,
    QwenImageOutputMode
)
from qwen_image_gen import (
    QwenImageGenModel,
    QwenImageGenProcessor,
    QwenImageGenWorker,
    QwenImageGenExecutor,
    create_qwen_image_config
)
import asyncio
import os
import sys
import torch
import numpy as np
from PIL import Image
from typing import Optional

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))


def setup_environment():
    """è®¾ç½®ç¯å¢ƒå˜é‡å’Œé…ç½®"""
    print("ğŸ”§ è®¾ç½®ç¯å¢ƒé…ç½®...")

    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ.setdefault("QWEN_MODEL_ID", "Qwen/Qwen-Image")
    os.environ.setdefault("QWEN_TRANSFORMER_SUBFOLDER", "transformer")
    os.environ.setdefault("QWEN_VAE_SUBFOLDER", "vae")
    os.environ.setdefault("QWEN_MAX_BATCH_SIZE", "2")
    os.environ.setdefault("QWEN_HEIGHT", "512")
    os.environ.setdefault("QWEN_WIDTH", "512")
    os.environ.setdefault("QWEN_GUIDANCE_SCALE", "4.0")
    os.environ.setdefault("QWEN_NUM_STEPS", "20")  # å‡å°‘æ­¥æ•°ç”¨äºæµ‹è¯•
    os.environ.setdefault("QWEN_DEBUG", "1")

    print("âœ… ç¯å¢ƒé…ç½®å®Œæˆ")


def create_dummy_embeddings(batch_size: int = 1, seq_len: int = 77, embed_dim: int = 768):
    """åˆ›å»ºæ¨¡æ‹Ÿçš„æ–‡æœ¬åµŒå…¥"""
    print(
        f"ğŸ“ åˆ›å»ºæ¨¡æ‹Ÿæ–‡æœ¬åµŒå…¥: batch_size={batch_size}, seq_len={seq_len}, embed_dim={embed_dim}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    dtype = torch.float16 if torch.cuda.is_available() else torch.float32

    # åˆ›å»ºæ¨¡æ‹Ÿçš„ CLIP é£æ ¼åµŒå…¥
    prompt_embeds = torch.randn(
        batch_size, seq_len, embed_dim,
        device=device, dtype=dtype
    )

    # åˆ›å»ºæ³¨æ„åŠ›æ©ç 
    prompt_mask = torch.ones(
        batch_size, seq_len,
        device=device, dtype=torch.bool
    )

    print(f"âœ… æ–‡æœ¬åµŒå…¥åˆ›å»ºå®Œæˆ: {prompt_embeds.shape}")
    return prompt_embeds, prompt_mask


def create_initial_latents(batch_size: int = 1, height: int = 512, width: int = 512):
    """åˆ›å»ºåˆå§‹å™ªå£°æ½œåœ¨è¡¨ç¤º"""
    print(f"ğŸ² åˆ›å»ºåˆå§‹å™ªå£°: batch_size={batch_size}, height={height}, width={width}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    dtype = torch.float16 if torch.cuda.is_available() else torch.float32

    # VAE ä¸‹é‡‡æ ·æ¯”ä¾‹é€šå¸¸æ˜¯ 8
    latent_height = height // 8
    latent_width = width // 8
    latent_channels = 4  # å…¸å‹çš„ VAE æ½œåœ¨é€šé“æ•°

    # åˆ›å»ºéšæœºå™ªå£°
    latents = torch.randn(
        batch_size, latent_channels, latent_height, latent_width,
        device=device, dtype=dtype
    )

    print(f"âœ… åˆå§‹å™ªå£°åˆ›å»ºå®Œæˆ: {latents.shape}")
    return latents


def create_timesteps(num_steps: int = 20, batch_size: int = 1):
    """åˆ›å»ºå»å™ªæ—¶é—´æ­¥"""
    print(f"â° åˆ›å»ºæ—¶é—´æ­¥: num_steps={num_steps}, batch_size={batch_size}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # çº¿æ€§è°ƒåº¦
    timesteps = torch.linspace(1.0, 0.0, steps=num_steps, device=device)
    # ç¼©æ”¾åˆ°å…¸å‹çš„æ‰©æ•£èŒƒå›´
    timesteps = (timesteps * 1000).long()

    # æ‰©å±•ä¸ºæ‰¹æ¬¡
    timesteps = timesteps.unsqueeze(0).expand(batch_size, -1)

    print(f"âœ… æ—¶é—´æ­¥åˆ›å»ºå®Œæˆ: {timesteps.shape}")
    return timesteps


def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("\nğŸ§ª æµ‹è¯•æ¨¡å‹åŠ è½½...")

    try:
        # åˆ›å»ºé…ç½®
        config = create_qwen_image_config(
            model_id="Qwen/Qwen-Image",
            transformer_subfolder="transformer",
            vae_subfolder="vae",
            max_batch_size=2,
            height=512,
            width=512
        )

        print(f"âœ… é…ç½®åˆ›å»ºæˆåŠŸ: {config.model_id}")

        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªæ˜¯æµ‹è¯•é…ç½®ï¼Œä¸å®é™…åŠ è½½æ¨¡å‹
        # å› ä¸ºæ¨¡å‹æ–‡ä»¶å¯èƒ½ä¸å­˜åœ¨ï¼Œä¼šå¯¼è‡´ä¸‹è½½å¤±è´¥
        print("âš ï¸  è·³è¿‡å®é™…æ¨¡å‹åŠ è½½ï¼ˆé¿å…ä¸‹è½½å¤§æ–‡ä»¶ï¼‰")

        return config

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        return None


def test_input_processing():
    """æµ‹è¯•è¾“å…¥å¤„ç†"""
    print("\nğŸ§ª æµ‹è¯•è¾“å…¥å¤„ç†...")

    try:
        # åˆ›å»ºå¤„ç†å™¨
        processor = QwenImageGenProcessor()

        # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥
        prompt_embeds, prompt_mask = create_dummy_embeddings()
        image_latents = create_initial_latents()
        timesteps = create_timesteps()

        # åˆ›å»º QwenImage è¾“å…¥
        qwen_inputs = QwenImageInputs(
            prompt_embeds=prompt_embeds,
            prompt_embeds_mask=prompt_mask,
            image_latents=image_latents,
            timesteps=timesteps,
            guidance_scale=4.0,
            num_inference_steps=20,
            task=QwenImageTask.TEXT_TO_IMAGE,
            output_mode=QwenImageOutputMode.PIXELS
        )

        print(f"âœ… è¾“å…¥å¤„ç†æµ‹è¯•æˆåŠŸ")
        print(f"   - æç¤ºåµŒå…¥: {qwen_inputs.prompt_embeds.shape}")
        print(f"   - å›¾åƒæ½œåœ¨: {qwen_inputs.image_latents.shape}")
        print(f"   - æ—¶é—´æ­¥: {qwen_inputs.timesteps.shape}")
        print(f"   - ä»»åŠ¡: {qwen_inputs.task}")
        print(f"   - è¾“å‡ºæ¨¡å¼: {qwen_inputs.output_mode}")

        return qwen_inputs

    except Exception as e:
        print(f"âŒ è¾“å…¥å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return None


def test_simple_generation():
    """æµ‹è¯•ç®€å•ç”Ÿæˆï¼ˆä¸ä¾èµ–å®é™…æ¨¡å‹ï¼‰"""
    print("\nğŸ§ª æµ‹è¯•ç®€å•ç”Ÿæˆ...")

    try:
        # åˆ›å»ºè¾“å…¥
        qwen_inputs = test_input_processing()
        if qwen_inputs is None:
            return None

        # æ¨¡æ‹Ÿç”Ÿæˆè¿‡ç¨‹
        print("ğŸ¨ æ¨¡æ‹Ÿå›¾åƒç”Ÿæˆè¿‡ç¨‹...")

        # è·å–åˆå§‹æ½œåœ¨è¡¨ç¤º
        latents = qwen_inputs.image_latents.clone()
        print(f"   åˆå§‹æ½œåœ¨è¡¨ç¤º: {latents.shape}")

        # æ¨¡æ‹Ÿå»å™ªè¿‡ç¨‹
        num_steps = qwen_inputs.num_inference_steps
        for i in range(min(5, num_steps)):  # åªè¿è¡Œå‰5æ­¥ç”¨äºæµ‹è¯•
            step = i + 1
            print(f"   æ­¥éª¤ {step}/{num_steps}: æ¨¡æ‹Ÿå»å™ª...")

            # æ¨¡æ‹Ÿå™ªå£°å‡å°‘
            noise_scale = 1.0 - (step / num_steps)
            noise = torch.randn_like(latents) * noise_scale * 0.1
            latents = latents - noise

            print(f"     æ½œåœ¨è¡¨ç¤ºèŒƒå›´: [{latents.min():.3f}, {latents.max():.3f}]")

        # æ¨¡æ‹Ÿ VAE è§£ç 
        print("ğŸ–¼ï¸  æ¨¡æ‹Ÿ VAE è§£ç ...")
        if qwen_inputs.output_mode in [QwenImageOutputMode.PIXELS, QwenImageOutputMode.BOTH]:
            # æ¨¡æ‹Ÿè§£ç åçš„åƒç´ å›¾åƒ
            batch_size, channels, height, width = latents.shape
            # ä¸Šé‡‡æ ·åˆ°åƒç´ ç©ºé—´
            pixel_height = height * 8
            pixel_width = width * 8
            pixel_channels = 3  # RGB

            pixels = torch.randn(
                batch_size, pixel_channels, pixel_height, pixel_width,
                device=latents.device, dtype=latents.dtype
            )

            # å½’ä¸€åŒ–åˆ° [0, 1] èŒƒå›´
            pixels = (pixels - pixels.min()) / (pixels.max() - pixels.min())

            print(f"   è§£ç åƒç´ å›¾åƒ: {pixels.shape}")
            print(f"   åƒç´ å€¼èŒƒå›´: [{pixels.min():.3f}, {pixels.max():.3f}]")

            return pixels
        else:
            print(f"   è¿”å›æ½œåœ¨è¡¨ç¤º: {latents.shape}")
            return latents

    except Exception as e:
        print(f"âŒ ç®€å•ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        return None


def save_test_image(pixels: torch.Tensor, filename: str = "test_generated_image.png"):
    """ä¿å­˜æµ‹è¯•å›¾åƒ"""
    print(f"\nğŸ’¾ ä¿å­˜æµ‹è¯•å›¾åƒ: {filename}")

    try:
        # è½¬æ¢ä¸º numpy æ•°ç»„
        if pixels.dim() == 4:
            pixels = pixels[0]  # å–ç¬¬ä¸€ä¸ªæ‰¹æ¬¡

        # è½¬æ¢ä¸º [H, W, C] æ ¼å¼
        pixels = pixels.permute(1, 2, 0).cpu().numpy()

        # ç¡®ä¿å€¼åœ¨ [0, 1] èŒƒå›´å†…
        pixels = np.clip(pixels, 0, 1)

        # è½¬æ¢ä¸º [0, 255] èŒƒå›´
        pixels = (pixels * 255).astype(np.uint8)

        # åˆ›å»º PIL å›¾åƒ
        image = Image.fromarray(pixels)

        # ä¿å­˜å›¾åƒ
        image.save(filename)

        print(f"âœ… å›¾åƒä¿å­˜æˆåŠŸ: {filename}")
        print(f"   å›¾åƒå°ºå¯¸: {image.size}")

    except Exception as e:
        print(f"âŒ å›¾åƒä¿å­˜å¤±è´¥: {e}")


def test_configuration():
    """æµ‹è¯•é…ç½®ç³»ç»Ÿ"""
    print("\nğŸ§ª æµ‹è¯•é…ç½®ç³»ç»Ÿ...")

    try:
        # æµ‹è¯•ç¯å¢ƒå˜é‡é…ç½®
        config = create_qwen_image_config()

        print("âœ… ç¯å¢ƒå˜é‡é…ç½®:")
        print(f"   - Transformer æ¨¡å‹: {config.transformer_model_id}")
        print(f"   - VAE æ¨¡å‹: {config.vae_model_id}")
        print(f"   - æœ€å¤§æ‰¹æ¬¡å¤§å°: {config.max_batch_size}")
        print(f"   - é»˜è®¤å°ºå¯¸: {config.default_height}x{config.default_width}")
        print(f"   - å¼•å¯¼æ¯”ä¾‹: {config.default_guidance_scale}")
        print(f"   - æ¨ç†æ­¥æ•°: {config.default_num_inference_steps}")
        print(f"   - è®¾å¤‡: {config.device}")
        print(f"   - æ•°æ®ç±»å‹: {config.dtype}")

        # æµ‹è¯• vLLM é…ç½®è½¬æ¢
        vllm_config_dict = config.to_vllm_config()
        print("\nâœ… vLLM é…ç½®è½¬æ¢:")
        for key, value in vllm_config_dict.items():
            print(f"   - {key}: {value}")

        # æµ‹è¯•é…ç½®éªŒè¯
        config.validate()
        print("âœ… é…ç½®éªŒè¯é€šè¿‡")

        return config

    except Exception as e:
        print(f"âŒ é…ç½®æµ‹è¯•å¤±è´¥: {e}")
        return None


def run_comprehensive_test():
    """è¿è¡Œç»¼åˆæµ‹è¯•"""
    print("ğŸš€ å¼€å§‹ QwenImage Generation ç»¼åˆæµ‹è¯•")
    print("=" * 60)

    # 1. è®¾ç½®ç¯å¢ƒ
    setup_environment()

    # 2. æµ‹è¯•é…ç½®
    config = test_configuration()
    if config is None:
        print("âŒ é…ç½®æµ‹è¯•å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return

    # 3. æµ‹è¯•æ¨¡å‹åŠ è½½
    test_model_loading()

    # 4. æµ‹è¯•è¾“å…¥å¤„ç†
    qwen_inputs = test_input_processing()
    if qwen_inputs is None:
        print("âŒ è¾“å…¥å¤„ç†æµ‹è¯•å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return

    # 5. æµ‹è¯•ç®€å•ç”Ÿæˆ
    result = test_simple_generation()
    if result is None:
        print("âŒ ç”Ÿæˆæµ‹è¯•å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return

    # 6. ä¿å­˜ç»“æœ
    if result.dim() == 4 and result.shape[1] == 3:  # åƒç´ å›¾åƒ
        save_test_image(result)

    print("\n" + "=" * 60)
    print("ğŸ‰ ç»¼åˆæµ‹è¯•å®Œæˆï¼")
    print("\nğŸ“‹ æµ‹è¯•æ€»ç»“:")
    print("   âœ… ç¯å¢ƒé…ç½®")
    print("   âœ… é…ç½®ç³»ç»Ÿ")
    print("   âœ… è¾“å…¥å¤„ç†")
    print("   âœ… ç”Ÿæˆæµç¨‹")
    print("   âœ… ç»“æœä¿å­˜")

    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("   1. ç¡®ä¿æœ‰å®é™…çš„ QwenImage æ¨¡å‹æ–‡ä»¶")
    print("   2. å®‰è£… diffusers: pip install diffusers")
    print("   3. è¿è¡Œå®Œæ•´çš„ vLLM é›†æˆæµ‹è¯•")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ QwenImage Generation æµ‹è¯•è„šæœ¬")
    print("=" * 60)

    # æ£€æŸ¥ CUDA å¯ç”¨æ€§
    if torch.cuda.is_available():
        print(f"ğŸ”¥ CUDA å¯ç”¨: {torch.cuda.get_device_name()}")
        print(
            f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ CPUï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")

    # è¿è¡Œæµ‹è¯•
    run_comprehensive_test()


if __name__ == "__main__":
    main()
