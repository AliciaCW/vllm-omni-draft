# 实验设计方案：基于vLLM的多模态图像生成/编辑任务性能评估

## 实验目标

**核心评估维度**：
1. **吞吐量（Throughput）**：每秒处理请求数（QPS）
2. **延迟（Latency）**：平均延迟、p50、p99尾延迟
3. **Token处理速率**：每秒处理token数量（tokens/s）

**对比方案**：
- **基线方法**：vLLM（原生方案）
- **优化方案**：自研优化方案

**任务类型**：图像生成/编辑任务（text + image输入）

---

## 需要明确的信息

### 1. 模型与数据集（必需）

#### 模型选择
- 具体使用哪些多模态生成模型？
  - LLaVA, BLIP, InstructBLIP？
  - Stable Diffusion + CLIP？
  - 其他图像生成模型（SDXL, DALL-E, Imagen等）？
  
- 模型规模和版本？
- 是否需要量化（FP8, INT8等）？

#### 数据集设计
- 使用什么图像数据集？
  - 公开数据集（COCO, Open Images, CC3M等）？
  - 合成数据集？
  - 生产环境真实数据？
  
- 图像规格：
  - 分辨率范围（如256x256, 512x512, 1024x1024）
  - 图像尺寸分布（固定还是可变）
  
- 文本提示分布：
  - 提示词长度范围
  - 简单提示vs复杂提示的占比

### 2. 工作负载配置（必需）

#### 请求模式
- 纯文本请求 vs 图像+文本请求占比（如50%-50%, 30%-70%）
- 图像生成任务 vs 图像编辑任务占比？
- 各任务的具体操作（如：生成、编辑、inpaint、outpaint）

#### 并发负载
- 预期最大并发数？
- 负载分布：
  - 恒定负载
  - 突增/脉冲负载
  - 重尾分布
- 持续实验时间？预热时间？

#### 批处理策略
- 是否启用batching？
- batch大小范围？
- 动态批处理策略？

### 3. 硬件与部署（必需）

#### 硬件配置
- GPU型号与数量（A100, H100, 4090等）
- 显存大小与配置
- CPU/内存配置
- 是否为单机还是分布式集群？

#### 部署方式
- 单GPU vs 多GPU（张量并行、流水线并行）
- vLLM版本与配置参数：
  - max_model_len
  - gpu_memory_utilization
  - max_num_seqs
  - 其他重要参数

### 4. 新优化方法细节（关键）

#### 优化方向
需要明确优化方案的至少一个技术方向：

**A. 架构优化**
- 是否采用ModServe式的阶段解耦？
- 具体哪些阶段需要解耦（图像编码、文本编码、生成、后处理）？

**B. 调度优化**
- 模态感知调度？
- 资源预分配策略？
- 请求优先级调度？

**C. 内存优化**
- 类似PagedAttention的图像分页？
- KV cache优化策略？
- 图像特征缓存？

**D. 批处理优化**
- 动态批处理策略？
- 模态感知批处理？
- Prefill/Decode分离？

**E. 其他**
- 预计算/预取？
- 流水线并行优化？
- 量化策略？

### 5. 指标采集与统计（必需）

#### 指标定义
- **吞吐量**：
  - QPS计算方式（requests/s）
  - 是否区分任务类型统计？
  
- **延迟**：
  - 端到端延迟 vs 模型推理延迟
  - 延迟分位数（p50, p90, p95, p99）
  
- **Token速率**：
  - Token定义（文本token + 图像token的统一计算方式）
  - 是否需要区分输入token vs 生成token？

#### 数据采集
- 使用什么工具监控？
- 采样频率？
- 实验重复次数与置信区间？

### 6. 实验对比场景（必需）

建议至少包含以下对比：

1. **纯vLLM方案**（基线）
   - 标准vLLM配置
   
2. **优化方案**
   - 不同优化参数下的性能

3. **理想上限**（可选）
   - 单GPU串行处理理论最大吞吐

### 7. 任务具体定义

#### 图像生成任务
- 输入：纯文本提示
- 输出：生成图像
- 评估：生成速度、图像质量（可选）

#### 图像编辑任务
- 输入：原图像 + 文本指令
- 输出：编辑后图像
- 评估：处理速度、编辑质量（可选）

---

## 初步实验框架建议

### Phase 1: 微基准测试（Micro-benchmarks）
**目标**：测试单任务性能

配置：
- 单个GPU
- 固定图像分辨率
- 固定文本长度
- 恒定负载

测量：
- 单请求延迟
- 批处理吞吐量
- Token处理速率

### Phase 2: 混合负载测试
**目标**：测试实际场景性能

配置：
- 多GPU部署
- 混合请求（生成 + 编辑）
- 变长输入（分辨率、文本长度变化）
- 动态负载

测量：
- 整体吞吐量
- 延迟分布（p50/p99）
- 资源利用率
- 与基线对比

### Phase 3: 压力测试（可选）
**目标**：测试极限性能

配置：
- 突增负载
- 大规模并发

测量：
- 系统稳定性
- SLO满足情况

---

## 待补充信息总结

### 高优先级（实验前必须确定）
1. ✅ 具体使用的模型（名称、版本、规模）
2. ✅ 数据集来源与规格
3. ✅ 实验硬件配置
4. ✅ 优化方法的技术细节（至少明确一个优化方向）
5. ✅ 工作负载设计（请求类型、并发数、分布）

### 中优先级（影响实验深度）
6. 批处理策略配置
7. 延迟/吞吐量的具体统计方法
8. vLLM详细配置参数
9. 实验重复次数与置信区间要求

### 低优先级（可选增强）
10. 图像质量评估方法
11. 成本分析
12. 资源利用率监控细节

---

## 下一步行动

1. **模型选择**：确定具体使用哪些多模态模型
2. **优化方案**：明确至少一个优化技术方向
3. **数据集**：选择合适的图像数据集
4. **硬件**：确认可用的GPU资源

建议优先确定**模型**和**优化方向**，这是实验设计的前提。

---

## ModServe 可计算的指标与推导方法

基于 ModServe 的方法论，以下数据可以通过**计算方法**获得（无需完整实验）：

### 1. 理论性能上限（可直接计算）

#### **峰值吞吐量计算**
```python
# 基于阶段解耦的理论模型
理论吞吐量 = 1 / (∑ 各阶段延迟)

例如：
- 阶段1（图像编码）：50ms
- 阶段2（文本编码）：30ms  
- 阶段3（融合推理）：200ms
- 阶段4（图像生成）：500ms
理论单请求延迟 = 780ms
理论吞吐量 = 1.28 QPS（单GPU）
```

**可计算的维度**：
- 单阶段瓶颈识别
- 理论最大QPS
- 内存使用上限

#### **资源利用率估算**
```
利用率 = 实际吞吐量 / 理论吞吐量

例如：
- 理论吞吐量：1.28 QPS
- 实际吞吐量：0.8 QPS
- 利用率：62.5%
```

### 2. 延迟分解分析（可计算）

基于 ModServe 的阶段解耦思想：

#### **各阶段延迟占比**
```
总延迟 = T_preprocess + T_encode_image + T_encode_text + T_inference + T_generation + T_postprocess

可计算：
- 各阶段占比
- 瓶颈识别
- 并行潜力分析
```

**推导目标**：
- 优化潜力评估
- 并行的收益预期
- 缓存策略有效性

### 3. 工作负载分布建模（可计算/合成）

#### **重尾分布参数**
ModServe 发现的特性，可通过数学建模：

```python
# 图像尺寸分布（重尾）
图像尺寸 ~ Pareto分布(alpha=2.5, x_min=256)

# 请求间隔（突发性）
请求间隔 ~ Gamma分布(shape=k, rate=λ)

# 负载突发模式
负载(t) = 基础负载 + 突发负载 × Poisson(λ)
```

**可计算**：
- 不同分布对性能的影响
- 尾延迟（p99）的数学预期
- 所需GPU资源的概率分布

### 4. 批处理收益预估（可计算）

#### **动态批处理理论增益**
```python
# 理想批处理吞吐量
批处理增益 = batch_size / (1 + 批处理开销系数)

例如：
- batch_size = 8
- 开销系数 = 0.3
- 增益 = 8 / 1.3 ≈ 6.15x
```

**可计算**：
- 最优batch size范围
- 批处理延迟增长
- 内存增加vs吞吐量提升的权衡

### 5. 成本分析（可直接计算）

#### **成本效率比**
```
单位请求成本 = (GPU数量 × 每小时成本 × 运行时间) / 处理请求总数

优化率 = (基线成本 - 优化方案成本) / 基线成本 × 100%
```

**可计算的维度**：
- GPU时成本
- 每分钟成本
- ROI（投资回报率）
- 不同配置的成本对比

### 6. 通过理论推导得出的数据

无需完整实验即可得到：

#### **A. 架构对比分析**
```
Decoder-only架构：
- 同步推理延迟 = max(图像编码, 文本编码) + 生成延迟

Cross-attention架构：
- 异步潜在收益 = 图像编码提前 + 并行编码

可计算相对性能差异（≈20-40%）
```

#### **B. 内存使用预估**
```
总显存 = 模型权重 + KV Cache + 激活值 + 临时buffer

图像编码器显存 = 图像特征 × batch_size × 特征维度 × 4 bytes
KV Cache = 2 × seq_len × hidden_size × num_layers × 4 bytes

可直接计算不同配置下的显存需求
```

#### **C. 并行策略有效性**
```
张量并行效率 = 1 / (1 + 通信开销系数 × 并发数)

流水线并行效率 = stage_num / (stage_num + bubble_latency)

可计算理论加速比
```

### 7. 实验前可预测的关键指标

通过计算方法可直接得出：

| 指标类型       | 可计算性   | 计算依据              |
| -------------- | ---------- | --------------------- |
| **理论吞吐量** | ✅ 高精度   | 阶段延迟累加          |
| **显存需求**   | ✅ 精确     | 模型规格 + 批处理size |
| **成本预估**   | ✅ 准确     | GPU价格 × 运行时间    |
| **批处理增益** | ✅ 理论值   | 数学建模              |
| **并行效率**   | ⚠️ 估算     | 通信模型              |
| **尾延迟**     | ⚠️ 分布假设 | 统计建模              |
| **实际吞吐量** | ❌ 需实验   | 受多种因素影响        |
| **真实延迟**   | ❌ 需实验   | 系统开销未知          |

### 8. 推荐的计算方法应用

在实验前，可以先计算：

1. **硬件需求分析**
   - 不同batch size下的显存占用
   - 理论最大并发数
   - GPU数量需求估算

2. **性能预估**
   - 理论吞吐量上限
   - 优化方案的理论增益
   - 不同配置的性能对比

3. **成本预算**
   - 测试所需GPU时数
   - 不同方案的运行成本
   - 预期ROI计算

4. **实验设计优化**
   - 最小实验配置
   - 关键参数范围
   - 采样策略设计

### 9. 具体可执行的推导步骤

```
步骤1：模型规格 → 单请求延迟分解
       ↓
步骤2：硬件配置 → 理论并发数
       ↓
步骤3：负载建模 → 吞吐量分布
       ↓
步骤4：成本计算 → 预算规划
       ↓
步骤5：实验验证 → 与理论对比
```

---

## 总结：哪些数据不需要完整实验

| 类型             | 可计算性     | 方法               |
| ---------------- | ------------ | ------------------ |
| 理论性能上限     | ✅ 高精度     | 阶段延迟分解       |
| 显存需求         | ✅ 精确       | 参数规模计算       |
| 成本预估         | ✅ 准确       | 资源 × 时间 × 价格 |
| 批处理增益理论值 | ✅ 准确       | 并行效率模型       |
| 瓶颈识别         | ✅ 准确       | 各阶段延迟占比     |
| 资源利用率估算   | ⚠️ 需简化假设 | 理论vs实际比率     |
| 尾延迟分布       | ⚠️ 需统计模型 | 概率分布建模       |
| 真实QPS          | ❌ 需实验     | 受实际系统影响     |
| SLO满足率        | ❌ 需实验     | 真实负载测试       |

**建议**：先通过理论计算做**预评估**，再用实验**验证**，这样可以：
- 节省实验资源
- 优化实验设计
- 快速识别可行方案



## ar & dit
首先，AR部分（自回归模型，如Transformer）的显存占用通常包括：

模型权重：参数数量乘以参数所占字节数。

KV缓存：用于自注意力机制，缓存过去键值对，计算公式为：2 × batch_size × seq_len × layers × hidden_dim × 数据类型大小。

激活值：前向传播过程中产生的中间结果，其大小与batch_size、序列长度、隐藏维度等成正比。

框架开销：深度学习框架本身需要的一些额外显存。

其次，DiT部分（扩散Transformer）的显存占用包括：

模型权重：参数数量乘以参数所占字节数。

激活值：前向传播中的中间激活值，计算公式为：batch_size × seq_len × hidden_dim × 激活值系数 × 数据类型大小。注意，这里的seq_len是图像patch序列长度。

KV缓存：同样用于自注意力机制，计算公式与AR部分相同，但序列长度是图像patch序列长度。

扩散过程状态：在扩散过程中需要保存的中间状态，比如多个时间步的噪声预测、潜在状态等。计算公式为：batch_size × num_timesteps_kept × latent_dim × 数据类型大小。

框架开销。

用户给出的公式在结构上是合理的，但需要注意以下几点：

在DiT部分，激活值的计算中有一个“激活值系数”，这个系数通常包括多个部分（如Q、K、V、FFN等），需要根据模型结构详细计算。但为了简化，可以给出一个经验值或范围。

对于DiT，序列长度seq_len是图像patch序列长度，即(image_size / patch_size)^2。

扩散过程状态中，num_timesteps_kept表示在采样过程中需要保留状态的时间步数。这取决于采样方法，例如DDPM需要保留所有时间步，而DDIM可能只需要保留部分。

注意，AR和DiT可能是两个独立的模型，因此总显存占用可能是两者之和（如果同时加载在GPU上）。但在分离架构中，可能可以分别部署在不同的设备上，所以需要分别计算。

另外，如果AR和DiT之间需要传递数据（例如AR生成的文本作为DiT的条件），则可能还有额外的显存开销（例如存储条件张量）。

框架开销通常是一个固定值，例如0.5-1GB，具体取决于框架和模型加载方式。